[
  {
    "objectID": "Architecture.html",
    "href": "Architecture.html",
    "title": "Architecture",
    "section": "",
    "text": "Architecture diagram"
  },
  {
    "objectID": "AMI.html",
    "href": "AMI.html",
    "title": "Custom AMI",
    "section": "",
    "text": "While AWS ParallelCluster allows the on-demand provisioning of software during cluster build and compute node spin up, the waiting time especially until a compute node is spun up and provisioned quickly becomes incompatible with a positive user experience. The use of a custom AMI is very helpful to reduce this time. The spin up time however cannot be reduced below approximately 4 to 5 minutes which is the minimum time needed to bring up an EC2 instance and let AWS parallelcluster configure it. In a previous system where all software on the compute node was deployed on-demand on a default AMI, the spin-up time was larger than 10 minutes."
  },
  {
    "objectID": "AMI.html#prerequisites",
    "href": "AMI.html#prerequisites",
    "title": "Custom AMI",
    "section": "3.1 Prerequisites",
    "text": "3.1 Prerequisites\nYou need to have access to a S3 bucket. In case you don’t have one available, you can create one using the AWS CLI, for example:\naws s3api create-bucket --bucket &lt;BUCKETNAME&gt; \\\n    --region &lt;REGION&gt; \\\n    --create-bucket-configuration LocationConstraint=&lt;REGION&gt;\nwhere &lt;BUCKETNAME&gt; is your desired name of the S3 bucket and &lt;REGION&gt; the AWS region you would like to deploy this bucket to."
  },
  {
    "objectID": "AMI.html#step-by-step-instruction",
    "href": "AMI.html#step-by-step-instruction",
    "title": "Custom AMI",
    "section": "3.2 Step by step instruction",
    "text": "3.2 Step by step instruction\n\nMake sure you have read/write access to a S3 bucket where you can store transient data and scripts. This bucket is referenced as &lt;BUCKETNAME&gt; hereafter.\nIdentify the appropriate pcluster AMI (cf. Section 3.3.4) and update ParentImage in image-config.yaml\nCheck versions in install-image.sh (cf. Section 3.3.3) and adjust accordingly\nFinally, run\n\n./build-image.sh &lt;IMAGENAME&gt; [&lt;BUCKETNAME&gt;]\nwhere &lt;IMAGENAME&gt; is the desired name of the new AMI and &lt;BUCKETNAME&gt; the name of the S3 bucket, e.g. hpc-scripts1234. &lt;BUCKETNAME&gt; is an optional argument. If missing, the script will look in .bucket.default to read the default bucket name."
  },
  {
    "objectID": "AMI.html#other-useful-information-for-debugging-etc.",
    "href": "AMI.html#other-useful-information-for-debugging-etc.",
    "title": "Custom AMI",
    "section": "3.3 Other useful information for debugging etc.",
    "text": "3.3 Other useful information for debugging etc.\n\n3.3.1 Cleanup\nIf you want to get rid of all AVAILABLE images, run\nfor i in `pcluster list-images --image-status AVAILABLE | grep imageId | \\\n    awk '{print $2}' | sed 's#\"##g' | sed 's#,##'`; \\\n        do pcluster delete-image -i $i ; done\n\n\n3.3.2 Get information about image\npcluster describe-image -i &lt;IMAGENAME&gt;\npcluster list-image-log-streams -i &lt;IMAGENAME&gt;\npcluster get-image-log-events  -i &lt;IMAGENAME&gt; --log-stream-name &lt;AWSPCVERSION&gt;/1\nwhere &lt;AWSPCVERSION&gt; is the version of AWS parallelcluster used (e.g. 3.11.1) and &lt;IMAGENAME&gt; is the custom name of your AMI.\n\n\n3.3.3 Sample install-image.sh script\nThis is just the start of the script.\nR_VERSION_LIST=\"4.4.2 4.3.3 4.2.3 4.1.3 4.0.5\"\nR_VERSION_DEFAULT=4.4.2\n\nPYTHON_VERSION_LIST=\"3.12.7 3.11.10 3.10.15\"\nPYTHON_VERSION_DEFAULT=3.12.7\n\nQUARTO_VERSION=1.6.34\n\nAPPTAINER_VERSION=\"1.3.5\"\n\n\n3.3.4 Find out the ID of the desired pcluster AMI\nYou can check the AMI ID for the supported OS (Ubuntu 22.04 LTS (Jammy)) and architecture (x86_64) via\npcluster list-official-images | \\\n    jq '.images.[] | \n        select(.os==\"ubuntu2204\" and .architecture==\"x86_64\") | .amiId'"
  },
  {
    "objectID": "PWB.html",
    "href": "PWB.html",
    "title": "Next Generation Workbench integration into AWS ParallelCluster",
    "section": "",
    "text": "AWS ParallelCluster is a framework that allows for easy setup of HPC clusters with various schedulers. It takes a YAML file with all the necessary definitions and transforms that into Cloud Formation Code that then gets deployed into AWS.\nAt the moment it supports both SLURM and AWS Batch.\nPosit Workbench supports SLURM as a HPC back end via the SLURM Launcher. As a consequence, a github repository has been set up to highlight a possible way to integrate Posit Workbench with AWS ParallelCluster via the SLURM Launcher. The approach used there works but has several shortcomings:\n\nSetting Workbench on the head node where also the slurmctld and slurmdbd (SLURM Controller and Database) daemons are running makes this head node very vulnerable and a single point of failure.\nAll traffic will be routed through the head node\nThe head node does not only act as Workbench Server and runs the main SLURM daemons (see above), it is also used as a NFS server adding additional load (depending on the size and utilisation of the cluster) that could contribute to very bad user experience on the HPC cluster up to a crash of the same if resources are exhausted.\n\nThis document serves two purposes: Documenting the current setup used for Workbench benchmarking but also summarizing a potential reference architecture that overcomes some of the shortcomings of the current Workbench integration into AWS ParallelCluster with a focus on High(er) Availabiity."
  },
  {
    "objectID": "PWB.html#sec-auxiliary-services",
    "href": "PWB.html#sec-auxiliary-services",
    "title": "Next Generation Workbench integration into AWS ParallelCluster",
    "section": "3.1 Auxiliary services",
    "text": "3.1 Auxiliary services\nWhen using Posit Workbench for High Availability, the use of a PostgreSQL db is mandatory. Given the distributed nature of a HPC cluster, some kind of directory service for user management is needed. The directory service of choice here is AWS SimpleAD. In order to efficiently and reliably work with this directory service, an additional EC2 instance is spun up that is used to add new users to the directory. This so-called jump host is fully integrated into SimpleAD and runs a tool called adcli (Active Directory CLI tool) that facilitates the management of users in SimpleAD. Via the use of additional expect scripts, this tool is used to programmatically create users. All of those tools and services are orchestrated via Pulumi recipes\n\n3.1.1 Prerequisites\nYou will need to have\n\npulumi installed and configure so you can successfully create, run and modify pulumi stacks\njust installed locally\nssh client including the ssh-keygen utility\n\n\n\n3.1.2 How to setup\nIn the github repo, go tho the pulumi sub-folder. There, run the following commands\n# Let's add a new ssh key pair\njust key-pair-new\n\n# Create a new stack \npulumi stack init auxiliary-wb\n\n# Configure eMail address to ensure resources are properly tagged\npulumi config set email my-email@corp.co\n\n# add EC2 keypair via AWS CLI\naws ec2 import-key-pair --key-name `pulumi config get email`-keypair-for-pulumi --public-key-material `cat key.pem.pub| base64 ` \n\n# Finally start deployment of SimpleAD, PostgreSQL DB and Jump Host\n# Also create 500 users at the same time\njust up\nPlease be aware\n\nNaming of your stack (auxiliary-wb) can be changed to your preference\nMake sure to set your correct eMail address.\nIf you would like to use a different number of users, instead of just up run pulumi up -y and then just create-users X where X is the number of users you want to create.\nYou can change the default values for various parameters defined in Pulumi.yaml to your liking as well. Please do NOT change Domain - this is currently hard-coded into the AWS ParallelCluster setup. Anything else can be changed as you see fit.\n\nCurrent configurable parameters in the pulumi recipe\n\nPulumi recipe parameters\n\n\n\n\n\n\n\nParameter\nDescription\nDefault value\n\n\n\n\nregion\nAWS region\neu-west-1\n\n\nemail\neMail address of user\ntbd@tbc.com\n\n\nServerInstanceType\nInstance Type for the AD jumphost\nt3.medium\n\n\nami\nA valid AMI used to deploy on AD jumphost (must be Ubuntu 20.04 LTS)\nami-0d2a4a5d69e46ea0b\n\n\nDomain\nName of Domain to be used for AD\npwb.posit.co\n\n\nDomainPW\nPassword for the Administrator AD account\nTestme123!\n\n\ndb_username\nUser name for PostgreSQL DB\npwb_db_admin\n\n\ndb_password\nPassword for PostgreSQL DB\npwb_db_password\n\n\n\nOnce you successfully built everything, pulumi stack output` should report something like\nCurrent stack outputs (12):\n    OUTPUT                   VALUE\n    DomainPWARN              arn:aws:secretsmanager:eu-west-1:637485797898:secret:SimpleADPassword-2898387-BQn4mT\n    ad_access_url            d-93675e652d.awsapps.com\n    ad_dns_1                 172.31.33.122\n    ad_dns_2                 172.31.48.170\n    ad_jump_host_public_dns  ec2-52-16-178-244.eu-west-1.compute.amazonaws.com\n    ad_jump_host_public_ip   52.16.178.244\n    db_address               rsw-dbfee1a4f.clovh3dmuvji.eu-west-1.rds.amazonaws.com\n    db_endpoint              rsw-dbfee1a4f.clovh3dmuvji.eu-west-1.rds.amazonaws.com:5432\n    db_port                  5432\n    jump_host_dns            ec2-52-16-178-244.eu-west-1.compute.amazonaws.com\n    key_pair id              michael.mayer@posit.co-keypair-for-pulumi-1699956356\n    vpc_subnet               subnet-03259a81db5aec449\n\n\n3.1.3 Additional details\nUsers are created in the following way by default: User Name is positXXXX where XXXX is a 4-digit zero-padded number. Password is Testme1234. Those defaults can be changed in server-side-files/config/useradd.sh . The referenced script is using multi-threaded bash to speed up user creation. In order to prevent user creation from failing due to too many concurrent connections, it additionally runs pamtester to ensure the user is correctly created."
  },
  {
    "objectID": "PWB.html#sec-custom-ami",
    "href": "PWB.html#sec-custom-ami",
    "title": "Next Generation Workbench integration into AWS ParallelCluster",
    "section": "3.2 Custom AMI",
    "text": "3.2 Custom AMI"
  },
  {
    "objectID": "PWB.html#sec-aws-parallelcluster-build",
    "href": "PWB.html#sec-aws-parallelcluster-build",
    "title": "Next Generation Workbench integration into AWS ParallelCluster",
    "section": "3.3 AWS ParallelCluster",
    "text": "3.3 AWS ParallelCluster\n\n3.3.1 Introduction\nWith launching the cluster via AWS ParallelCluster, everything comes together.\n\n\n3.3.2 Python Virtual Env\nThe virtual environment for AWS Parallelcluster can be created from the base folder of the git repo via\npython -m venv .aws-pc-venv\nsource .aws-pc-venv/bin/activate \npip install -r requirements.txt\ndeactivate\nYou may want to add the patch described in Section 4.1 to ensure full functionality of workbench.\n\n\n3.3.3 Prerequisites\n\nPython Virtual Environment set up and activated (cf. Section 3.3.2).\nAuxiliary Services up and running (cf. Section 3.1)\nCustom AMI built (cf. Section 3.2)\nS3 bucket set up for temporarily hosting cluster deployment files and scripts\n\n\n\n3.3.4 Deployment instructions\n\nReview the cluster template in config/cluster-config-wb.tmpl and modify accordingly.\nReview the deploy.sh script and modify accordingly, especially\n\nCLUSTERNAME - a human readable name of your cluster\nS3_BUCKETNAME - The name of the S3 bucket you set up in Section 3.3.3\nSECURITYGROUP_RSW - a security group that should allow at least external access to port 443 and 8787 (the latter if no SSL is being used).\nAMI - the AMI created in ?@sec-how-to-build-a-custom-ami\nSINGULARITY_SUPPORT - if set true, Workbench will be configured for Singularity integration and two r-session-complete containers (Ubuntu Jammy and Cent OS 7 based) will be built. Please note that this significantly extends the spin-up time of the cluster.\n\n\n\n\n3.3.5 Default values for Cluster deployment\nFor the deploy.sh script, unless mentioned in step 2 of the deployment instructions (cf. Section 3.3.4), all relevant parameters are extracted from the pulumi deployment for the auxiliary services.\nThe default value in the cluster template config/cluster-config-wb.tmpl` are as follows\n\nEFS storage used for shared file systems needed by AWS ParallelCluster\nOne Head Node\n\nInstance t3.xlarge\n100 GB of local EBS storage\nScript install-pwb-config.sh triggered when head node is being deployed.\n\nCompute Nodes with\n\nScript config-compute.sh triggered when compute node starts.\nPartition all\n\nInstance t3.xlarge\nminimum/maximum number of instances: 1/10\n\nPartition gpu\n\nInstance p3.2xlarge\nminimum/maximum number of instances; 0/1\n\n\n2 Login Nodes with\n\nInstance t3.xlarge\nELB in front\n\nShared storage for /home - FsX for Lustre with capacity of 1.2 TB and deployment type SCRATCH_2\n\nAll of the above settings (Instance type, numbers, FsX size) can be changed as needed.\n\n\n3.3.6 Notes on install-pwb-config.sh and install-compute.sh\ninstall-pwb-config.sh mainly creates Posit Workbench configuration files and configures the workbench systemctl services rstudio-launcher and rstudio-server . It is only executed on the designated head node\n\nWorkbench uses /opt/parallelcluster/shared/rstudio/ as the base for its configuration (PWB_BASE_DIR). /opt/parallelcluster/shared` is already created by AWS ParallelCluster and shared across all nodes (head, login and compute) so we are making use of this functionality.\nconfiguration files are deployed in $PWB_BASE_DIR/etc/rstudio\nshared storage is configured in $PWB_BASE_DIR/shared\nR Versions file is configured in $PWB_BASE_DIR/shared/r-versions\nIn order to distinguish the head node from the login node, an empty file /etc/head-node is created. This is used in the cron job mentioned in Section 3.2 to help differentiate the login nodes from the head node.\n\nìnstall-compute.sh script detects the presence of a GPU and then automatically updates the NVIDIA/CUDA driver and installs the CuDNN library for distributed GPU computing. This is more a nice to have but is rsther useful for distributed tensorflow etc…\n\n\n\nArchitecture diagram\n\n\n\n\n3.3.7 Customisations on top of AWS ParallelCluster\n\n3.3.7.1 Elastic Load Balancer\nAWS ParallelCluster is setting up an ELB for the Login nodes and ensures that the desired number of login nodes is available at any given time. The ELB is by default listening on port 22 (ssh). In order to change that one would need to patch the python scripts a bit (patch supplied in Section 4.1)\nThis change is simple but will effectively disable the ability to ssh into the ELB. Typically however Workbench Users do not need ssh access to login nodes - if needed, they can open a termina within the RStudio IDE, for example.\nAn alternative would be to add a second ELB for Workbench but this would imply a significantly larger patch to AWS ParallelCluster.\n\n\n3.3.7.2 The “thing” with the Login Nodes\nAWS ParallelCluster introduced the ability to define separate login nodes in Version 3.7.0. This is great and replaces a rather complicated workaround that was in place until then. Unfortunately the team did not add the same features to the new Login Nodes such as OnNodeConfigured . We have raise a github issue which was acknowledged and the missing feature will be implemented in an upcoming release.\nAs a consequence we have implemented a workaround with a cron job that runs on all ParallelCluster managed nodes (Login, Head and Compute) every minute. A login node is detected if there is a NFS mount that contains the name login_node and if there is no file /etc/head-node (the latter would signal that this is a head node indeed). See Section 3.3.6 for additional information.\nUntil the github issue is fixed, we will have to live with this workaround."
  },
  {
    "objectID": "PWB.html#summary-and-conclusions",
    "href": "PWB.html#summary-and-conclusions",
    "title": "Next Generation Workbench integration into AWS ParallelCluster",
    "section": "3.4 Summary and Conclusions",
    "text": "3.4 Summary and Conclusions\nThis document describes a possibility on how to integrate Workbench and AWS ParallelCluster that allows for partial High Availability. The setup can tolerate login node failures and recover and as a consequence the workbench part is HA.\nThe main ingredients for this setup is the creation of a custom AMI with all the software needed (Workbench, R, Python, …) baked into a custom AMI that can be used for all the three node types (Login, Head and Compute Node).\nIn order to achieve this, some additional logic has to be implemented and some workarounds for missing features in AWS ParallelCluster be used.\nThe remaining issue is however the single head node which is a single point of failure (if the head node crashes, SLURM stops working).\n\n3.4.1 How to reach “full” HA\nAWS paralelcluster makes a clear distinction between Head and Login nodes. This is more than justified given the fact that the Head node not only runs slurmctld but also can act as a NFS server exporting file systems such as /home , /opt/slurm, … This makes the Head node a single point of failure from the perspective of the NFS server alone.\nWith the release of AWS ParallelCluster 3.8.0 (currently available as beta version), all the NFS file systems can be hosted on external EFS. This removes the single point of failure for the NFS server. There is a bug in the beta version where all but one file system can be hosted on EFS but this will be fixed in the official release of 3.8.0.\nOnce this is in place, the boundaries between the Login Nodes and Head Nodes will become much less clear. With adding additional logic, one can automatically start additional slurmctld processes on the login nodes and configure those hosts in the slurm configuration. If the head node then fails, a slurmctld of one of the compute nodes will take over. While adding additional slurmctld is fairly straightforward, there also is a need for regular checks if all the defined slurmctld` hosts are still up and running. If not, those need to be removed from the slurm config.\nThe complexity of establishing the above is fairly small but then it is another customisation we have to make and maintain. As long as this is only a posit internal solution, we should be ok.\nA drawback of having full HA as mentioned above however is that very likely the ParallelCluster API may become unuseable in case the head node is no longer available. Things like updating configuration and settings of the running cluster may no longer work. Whether this is needed in a productive cluster is another matter of debate."
  },
  {
    "objectID": "PWB.html#sec-patch-for-elb",
    "href": "PWB.html#sec-patch-for-elb",
    "title": "Next Generation Workbench integration into AWS ParallelCluster",
    "section": "4.1 Patch for ELB to listen on port 8787 instead of 22",
    "text": "4.1 Patch for ELB to listen on port 8787 instead of 22\ndiff -u --recursive pcluster/templates/cluster_stack.py pcluster.new/templates/cluster_stack.py\n--- pcluster/templates/cluster_stack.py 2023-11-22 12:25:53\n+++ pcluster.new/templates/cluster_stack.py 2023-11-22 15:11:48\n@@ -871,10 +871,10 @@\n     def _get_source_ingress_rule(self, setting):\n         if setting.startswith(\"pl\"):\n             return ec2.CfnSecurityGroup.IngressProperty(\n-                ip_protocol=\"tcp\", from_port=22, to_port=22, source_prefix_list_id=setting\n+                ip_protocol=\"tcp\", from_port=8787, to_port=8787, source_prefix_list_id=setting\n             )\n         else:\n-            return ec2.CfnSecurityGroup.IngressProperty(ip_protocol=\"tcp\", from_port=22, to_port=22, cidr_ip=setting)\n+            return ec2.CfnSecurityGroup.IngressProperty(ip_protocol=\"tcp\", from_port=8787, to_port=8787, cidr_ip=setting)\n \n     def _add_login_nodes_security_group(self):\n         login_nodes_security_group_ingress = [\ndiff -u --recursive pcluster/templates/login_nodes_stack.py pcluster.new/templates/login_nodes_stack.py\n--- pcluster/templates/login_nodes_stack.py 2023-11-22 12:25:53\n+++ pcluster.new/templates/login_nodes_stack.py 2023-11-22 15:11:19\n@@ -273,10 +273,10 @@\n             self,\n             f\"{self._pool.name}TargetGroup\",\n             health_check=elbv2.HealthCheck(\n-                port=\"22\",\n+                port=\"8787\",\n                 protocol=elbv2.Protocol.TCP,\n             ),\n-            port=22,\n+            port=8787,\n             protocol=elbv2.Protocol.TCP,\n             target_type=elbv2.TargetType.INSTANCE,\n             vpc=self._vpc,\n@@ -299,7 +299,7 @@\n             ),\n         )\n \n-        listener = login_nodes_load_balancer.add_listener(f\"LoginNodesListener{self._pool.name}\", port=22)\n+        listener = login_nodes_load_balancer.add_listener(f\"LoginNodesListener{self._pool.name}\", port=8787)\n         listener.add_target_groups(f\"LoginNodesListenerTargets{self._pool.name}\", target_group)\n         return login_nodes_load_balancer"
  }
]