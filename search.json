[
  {
    "objectID": "Architecture.html",
    "href": "Architecture.html",
    "title": "Architecture",
    "section": "",
    "text": "Architecture diagram"
  },
  {
    "objectID": "AMI.html",
    "href": "AMI.html",
    "title": "Custom AMI",
    "section": "",
    "text": "While AWS ParallelCluster allows the on-demand provisioning of software during cluster build and compute node spin up, the waiting time especially until a compute node is spun up and provisioned quickly becomes incompatible with a positive user experience. The use of a custom AMI is very helpful to reduce this time. The spin up time however cannot be reduced below approximately 4 to 5 minutes which is the minimum time needed to bring up an EC2 instance and let AWS parallelcluster configure it. In a previous system where all software on the compute node was deployed on-demand on a default AMI, the spin-up time was larger than 10 minutes."
  },
  {
    "objectID": "AMI.html#main-configuration",
    "href": "AMI.html#main-configuration",
    "title": "Custom AMI",
    "section": "2.1 Main configuration",
    "text": "2.1 Main configuration\nThe main configuration file is image-config.yaml in the image subfolder. The idea for the custom image is to use a very slim approach. The yaml file references a script install-image.sh (cf. Section 4.3) that calls a couple of additional scripts that facilitate the installation of R, Python, quarto and apptainer. Each individual script, stored in an S3 bucket, is called by the setup_something() bash function that will download the additional script and run with pre-defined parameters that define the selected list of versions for the respective software."
  },
  {
    "objectID": "AMI.html#r-and-python-configuration",
    "href": "AMI.html#r-and-python-configuration",
    "title": "Custom AMI",
    "section": "2.2 R and Python configuration",
    "text": "2.2 R and Python configuration\nThe R and Python installation scripts allow the installation of multiple Python and R versions using the Posit provided binaries. Each default version is symlinked into /usr/local/bin.\nEach R version (achieved by run.R)\n\nis configured to use public Posit Package Manager\nis configured to use CRAN and Bioconductor repositories\nhas all R packages needed for the RStudio IDE integration preinstalled into a site-library\nuses a time-based snapshot for CRAN that points to the snapshot that is at most 60 days older than the corresponding R release date\nuses a global renv cachethat points to /home/renv\nhas the renv-pak integration enabled\n\nThe Python versions are setup to\n\ninclude the jupyter/jupyterlab integration\ninclude the capability to publish to Posit Connect\nuse pypi from public Posit Package Manager"
  },
  {
    "objectID": "AMI.html#an-important-cron-job",
    "href": "AMI.html#an-important-cron-job",
    "title": "Custom AMI",
    "section": "2.3 An important cron job",
    "text": "2.3 An important cron job\nIn install-image.sh we also set up a cron job that will run /opt/rstudio/scripts/rc.pwb. When using the AMI, the cron job will check every minute whether the AMI is used on a login node or not. If on a login node, it will activate and start rstudio-server as well as rstudio-launcher. This has become necessary because AWS ParallelCluster does not support triggering of scripts upon the launch of login nodes (cf. https://github.com/aws/aws-parallelcluster/issues/5723)"
  },
  {
    "objectID": "AMI.html#prerequisites",
    "href": "AMI.html#prerequisites",
    "title": "Custom AMI",
    "section": "3.1 Prerequisites",
    "text": "3.1 Prerequisites\nYou need to have access to a S3 bucket. In case you don’t have one available, you can create one using the AWS CLI, for example:\naws s3api create-bucket --bucket &lt;BUCKETNAME&gt; \\\n    --region &lt;REGION&gt; \\\n    --create-bucket-configuration LocationConstraint=&lt;REGION&gt;\nwhere &lt;BUCKETNAME&gt; is your desired name of the S3 bucket and &lt;REGION&gt; the AWS region you would like to deploy this bucket to."
  },
  {
    "objectID": "AMI.html#step-by-step-instruction",
    "href": "AMI.html#step-by-step-instruction",
    "title": "Custom AMI",
    "section": "3.2 Step by step instruction",
    "text": "3.2 Step by step instruction\nIn the github repo, go tho the image sub-folder.\n\nMake sure you have read/write access to a S3 bucket where you can store transient data and scripts. This bucket is referenced as &lt;BUCKETNAME&gt; hereafter.\nIdentify the appropriate pcluster AMI (cf. Section 4.4) and update ParentImage in image-config.yaml\nCheck versions in install-image.sh (cf. Section 4.3) and adjust accordingly\nFinally, run\n\n./build-image.sh &lt;IMAGENAME&gt; [&lt;BUCKETNAME&gt;]\nwhere &lt;IMAGENAME&gt; is the desired name of the new AMI and &lt;BUCKETNAME&gt; the name of the S3 bucket, e.g. hpc-scripts1234. &lt;BUCKETNAME&gt; is an optional argument. If missing, the script will look in .bucket.default to read the default bucket name."
  },
  {
    "objectID": "AMI.html#ami-cleanup",
    "href": "AMI.html#ami-cleanup",
    "title": "Custom AMI",
    "section": "4.1 AMI Cleanup",
    "text": "4.1 AMI Cleanup\nIf you want to get rid of all AVAILABLE images, run\nfor i in `pcluster list-images --image-status AVAILABLE | grep imageId | \\\n    awk '{print $2}' | sed 's#\"##g' | sed 's#,##'`; \\\n        do pcluster delete-image -i $i ; done"
  },
  {
    "objectID": "AMI.html#get-information-about-image",
    "href": "AMI.html#get-information-about-image",
    "title": "Custom AMI",
    "section": "4.2 Get information about image",
    "text": "4.2 Get information about image\npcluster describe-image -i &lt;IMAGENAME&gt;\npcluster list-image-log-streams -i &lt;IMAGENAME&gt;\npcluster get-image-log-events  -i &lt;IMAGENAME&gt; --log-stream-name &lt;AWSPCVERSION&gt;/1\nwhere &lt;AWSPCVERSION&gt; is the version of AWS parallelcluster used (e.g. 3.11.1) and &lt;IMAGENAME&gt; is the custom name of your AMI."
  },
  {
    "objectID": "AMI.html#sec-install-image-sh",
    "href": "AMI.html#sec-install-image-sh",
    "title": "Custom AMI",
    "section": "4.3 Sample install-image.sh script",
    "text": "4.3 Sample install-image.sh script\nThis is just the start of the script.\nR_VERSION_LIST=\"4.4.2 4.3.3 4.2.3 4.1.3 4.0.5\"\nR_VERSION_DEFAULT=4.4.2\n\nPYTHON_VERSION_LIST=\"3.12.7 3.11.10 3.10.15\"\nPYTHON_VERSION_DEFAULT=3.12.7\n\nQUARTO_VERSION=1.6.34\n\nAPPTAINER_VERSION=\"1.3.5\""
  },
  {
    "objectID": "AMI.html#sec-pcluster-ami",
    "href": "AMI.html#sec-pcluster-ami",
    "title": "Custom AMI",
    "section": "4.4 Find out the ID of the desired pcluster AMI",
    "text": "4.4 Find out the ID of the desired pcluster AMI\nYou can check the AMI ID for the supported OS (Ubuntu 22.04 LTS (Jammy)) and architecture (x86_64) via\npcluster list-official-images | \\\n    jq '.images.[] | \n        select(.os==\"ubuntu2204\" and .architecture==\"x86_64\") | .amiId'"
  },
  {
    "objectID": "AMI.html#find-the-latest-default-ubuntu-22.04-ami",
    "href": "AMI.html#find-the-latest-default-ubuntu-22.04-ami",
    "title": "Custom AMI",
    "section": "4.5 Find the latest default Ubuntu 22.04 AMI",
    "text": "4.5 Find the latest default Ubuntu 22.04 AMI\naws ec2 describe-images \\\n    --owners 099720109477 \\\n    --filters \"Name=name,Values=ubuntu/images/hvm-ssd/ubuntu-jammy-22.04-amd64-server-*\" \\\n              \"Name=architecture,Values=x86_64\" \\\n              \"Name=virtualization-type,Values=hvm\" \\\n    --query 'Images[*].[CreationDate,ImageId]' \\\n    --output text | sort | tail -1 | awk '{print $2}'"
  },
  {
    "objectID": "Pulumi.html",
    "href": "Pulumi.html",
    "title": "Pulumi",
    "section": "",
    "text": "When using Posit Workbench for High Availability, the use of a PostgreSQL db is mandatory. Given the distributed nature of a HPC cluster, some kind of directory service for user management is needed. The directory service of choice here is AWS SimpleAD. In order to efficiently and reliably work with this directory service, an additional EC2 instance is spun up that is used to add new users to the directory. This so-called jump host is fully integrated into SimpleAD and runs a tool called adcli (Active Directory CLI tool) that facilitates the management of users in SimpleAD. Via the use of additional expect scripts, this tool is used to programmatically create users. All of those tools and services are orchestrated via Pulumi recipes"
  },
  {
    "objectID": "Pulumi.html#additionl-information",
    "href": "Pulumi.html#additionl-information",
    "title": "Pulumi",
    "section": "3.1 Additionl information",
    "text": "3.1 Additionl information\nPlease be aware\n\nNaming of your stack (auxiliary-wb) can be changed to your preference\nMake sure to set your correct eMail address.\nIf you would like to use a different number of users, run just create-users X where X is the number of users you want to create.\nYou can change the default values for various parameters defined in Pulumi.yaml to your liking as well. Please do NOT change Domain - this is currently hard-coded into the AWS ParallelCluster setup. Anything else can be changed as you see fit.\nUsers are created in the following way by default: User Name is positXXXX where XXXX is a 4-digit zero-padded number. Password is chosen as per {ref:sec-passwords}. Those defaults can be changed in server-side-files/config/useradd.sh . The referenced script is using multi-threaded bash to speed up user creation. In order to prevent user creation from failing due to too many concurrent connections, it additionally runs pamtester to ensure the user is correctly created."
  },
  {
    "objectID": "Pulumi.html#sec-passwords",
    "href": "Pulumi.html#sec-passwords",
    "title": "Pulumi",
    "section": "3.2 Defining “good”/secure passwords",
    "text": "3.2 Defining “good”/secure passwords\nSince this deployment will be done in the AWS cloud with some instances exposed to the public internet, using secure passwords is a necessity. As a consequence, all passwords using random strings with 16 characters with a minumum of 1 lower case and 1 upper case letter as well 1 number.\nPasswords that are needed by the ParallelCluster deployment (e.g. for the SLURM or PostgreSQL DB) are additionally stored as a secret in AWS Secrets Manager. Same is true for the common password for all positXXXX users.\nCurrent configurable parameters in the pulumi recipe\n\nPulumi recipe parameters\n\n\n\n\n\n\n\nParameter\nDescription\nDefault value\n\n\n\n\nregion\nAWS region\neu-west-1\n\n\nemail\neMail address of user\ntbd@tbc.com\n\n\nServerInstanceType\nInstance Type for the AD jumphost\nt3.medium\n\n\nDomain\nName of Domain to be used for AD\npwb.posit.co\n\n\nslurm_db_username\nUser name for MySQL DB used for SLURM accounting\nslurm_db_admin\n\n\nrsw_db_username\nUser name for PostgreSQL DB for Workbench metadata\npwb_db_admin\n\n\n\nOnce you successfully built everything, pulumi stack output` should report something like\nCurrent stack outputs (36):\n    OUTPUT                               VALUE\n    ad_access_url                        d-9367b69bb0.awsapps.com\n    ad_dns_1                             10.0.12.162\n    ad_dns_2                             10.0.129.28\n    ad_password                          [secret]\n    ad_password_arn                      arn:aws:secretsmanager:eu-west-1:637485797898:secret:SimpleADPassword-testing-2e1b657-Q2G38a\n    ami_id                               ami-00e528f20622b7c63\n    billing_code                         me\n    elb_access                           arn:aws:iam::637485797898:policy/elbaccess-20a0cab\n    jump_host_dns                        ec2-3-249-65-17.eu-west-1.compute.amazonaws.com\n    jump_host_public_ip                  3.249.65.17\n    key_pair id                          michael.mayer3@posit.co-keypair-for-pulumi\n    posit_user_pass                      [secret]\n    posit_user_pass_arn                  arn:aws:secretsmanager:eu-west-1:637485797898:secret:PositUserPassword-testing-3b66e30-bIFVvn\n    rsw_db_address                       rsw-dbdf49494.clovh3dmuvji.eu-west-1.rds.amazonaws.com\n    rsw_db_endpoint                      rsw-dbdf49494.clovh3dmuvji.eu-west-1.rds.amazonaws.com:5432\n    rsw_db_name                          pwb\n    rsw_db_pass                          [secret]\n    rsw_db_port                          5432\n    rsw_db_user                          pwb_db_admin\n    rsw_security_group                   sg-077c470194ee6b798\n    rsw_security_group_db                sg-0d7b12857f91b688a\n    s3_bucket_id                         hpc-scripts-testing-f75b7d7\n    secure_cookie_key                    [secret]\n    slurm_db_address                     slurm-db7c04b6e.clovh3dmuvji.eu-west-1.rds.amazonaws.com\n    slurm_db_endpoint                    slurm-db7c04b6e.clovh3dmuvji.eu-west-1.rds.amazonaws.com:3306\n    slurm_db_name                        slurm\n    slurm_db_pass                        [secret]\n    slurm_db_pass_arn                    arn:aws:secretsmanager:eu-west-1:637485797898:secret:SlurmDBPassword-testing-04dc72f-syQYzS\n    slurm_db_port                        3306\n    slurm_db_user                        slurm_db_admin\n    slurm_security_group_db              sg-02535d06fd5bb5503\n    ssh_security_group                   sg-0f6e1c077d4f3e04c\n    stack_name                           testing\n    vpc_public_subnet                    subnet-0bb915288a434510b"
  },
  {
    "objectID": "PWB.html",
    "href": "PWB.html",
    "title": "Next Generation Workbench integration into AWS ParallelCluster",
    "section": "",
    "text": "AWS ParallelCluster is a framework that allows for easy setup of HPC clusters with various schedulers. It takes a YAML file with all the necessary definitions and transforms that into Cloud Formation Code that then gets deployed into AWS.\nAt the moment it supports both SLURM and AWS Batch.\nPosit Workbench supports SLURM as a HPC back end via the SLURM Launcher. As a consequence, a github repository has been set up to highlight a possible way to integrate Posit Workbench with AWS ParallelCluster via the SLURM Launcher. The approach used there works but has several shortcomings:\n\nSetting Workbench on the head node where also the slurmctld and slurmdbd (SLURM Controller and Database) daemons are running makes this head node very vulnerable and a single point of failure.\nAll traffic will be routed through the head node\nThe head node does not only act as Workbench Server and runs the main SLURM daemons (see above), it is also used as a NFS server adding additional load (depending on the size and utilisation of the cluster) that could contribute to very bad user experience on the HPC cluster up to a crash of the same if resources are exhausted.\n\nThis document serves two purposes: Documenting the current setup used for Workbench benchmarking but also summarizing a potential reference architecture that overcomes some of the shortcomings of the current Workbench integration into AWS ParallelCluster with a focus on High(er) Availabiity."
  },
  {
    "objectID": "PWB.html#sec-auxiliary-services",
    "href": "PWB.html#sec-auxiliary-services",
    "title": "Next Generation Workbench integration into AWS ParallelCluster",
    "section": "3.1 Auxiliary services",
    "text": "3.1 Auxiliary services\nWhen using Posit Workbench for High Availability, the use of a PostgreSQL db is mandatory. Given the distributed nature of a HPC cluster, some kind of directory service for user management is needed. The directory service of choice here is AWS SimpleAD. In order to efficiently and reliably work with this directory service, an additional EC2 instance is spun up that is used to add new users to the directory. This so-called jump host is fully integrated into SimpleAD and runs a tool called adcli (Active Directory CLI tool) that facilitates the management of users in SimpleAD. Via the use of additional expect scripts, this tool is used to programmatically create users. All of those tools and services are orchestrated via Pulumi recipes\n\n3.1.1 Prerequisites\nYou will need to have\n\npulumi installed and configure so you can successfully create, run and modify pulumi stacks\njust installed locally\nssh client including the ssh-keygen utility\n\n\n\n3.1.2 How to setup\nIn the github repo, go tho the pulumi sub-folder. There, run the following commands\n# Let's add a new ssh key pair\njust key-pair-new\n\n# Create a new stack \npulumi stack init auxiliary-wb\n\n# Configure eMail address to ensure resources are properly tagged\npulumi config set email my-email@corp.co\n\n# add EC2 keypair via AWS CLI\naws ec2 import-key-pair --key-name `pulumi config get email`-keypair-for-pulumi --public-key-material `cat key.pem.pub| base64 ` \n\n# Finally start deployment of SimpleAD, PostgreSQL DB and Jump Host\n# Also create 500 users at the same time\njust up\nPlease be aware\n\nNaming of your stack (auxiliary-wb) can be changed to your preference\nMake sure to set your correct eMail address.\nIf you would like to use a different number of users, instead of just up run pulumi up -y and then just create-users X where X is the number of users you want to create.\nYou can change the default values for various parameters defined in Pulumi.yaml to your liking as well. Please do NOT change Domain - this is currently hard-coded into the AWS ParallelCluster setup. Anything else can be changed as you see fit.\n\nCurrent configurable parameters in the pulumi recipe\n\nPulumi recipe parameters\n\n\n\n\n\n\n\nParameter\nDescription\nDefault value\n\n\n\n\nregion\nAWS region\neu-west-1\n\n\nemail\neMail address of user\ntbd@tbc.com\n\n\nServerInstanceType\nInstance Type for the AD jumphost\nt3.medium\n\n\nami\nA valid AMI used to deploy on AD jumphost (must be Ubuntu 20.04 LTS)\nami-0d2a4a5d69e46ea0b\n\n\nDomain\nName of Domain to be used for AD\npwb.posit.co\n\n\nDomainPW\nPassword for the Administrator AD account\nTestme123!\n\n\ndb_username\nUser name for PostgreSQL DB\npwb_db_admin\n\n\ndb_password\nPassword for PostgreSQL DB\npwb_db_password\n\n\n\nOnce you successfully built everything, pulumi stack output` should report something like\nCurrent stack outputs (12):\n    OUTPUT                   VALUE\n    DomainPWARN              arn:aws:secretsmanager:eu-west-1:637485797898:secret:SimpleADPassword-2898387-BQn4mT\n    ad_access_url            d-93675e652d.awsapps.com\n    ad_dns_1                 172.31.33.122\n    ad_dns_2                 172.31.48.170\n    ad_jump_host_public_dns  ec2-52-16-178-244.eu-west-1.compute.amazonaws.com\n    ad_jump_host_public_ip   52.16.178.244\n    db_address               rsw-dbfee1a4f.clovh3dmuvji.eu-west-1.rds.amazonaws.com\n    db_endpoint              rsw-dbfee1a4f.clovh3dmuvji.eu-west-1.rds.amazonaws.com:5432\n    db_port                  5432\n    jump_host_dns            ec2-52-16-178-244.eu-west-1.compute.amazonaws.com\n    key_pair id              michael.mayer@posit.co-keypair-for-pulumi-1699956356\n    vpc_subnet               subnet-03259a81db5aec449\n\n\n3.1.3 Additional details\nUsers are created in the following way by default: User Name is positXXXX where XXXX is a 4-digit zero-padded number. Password is Testme1234. Those defaults can be changed in server-side-files/config/useradd.sh . The referenced script is using multi-threaded bash to speed up user creation. In order to prevent user creation from failing due to too many concurrent connections, it additionally runs pamtester to ensure the user is correctly created."
  },
  {
    "objectID": "PWB.html#sec-custom-ami",
    "href": "PWB.html#sec-custom-ami",
    "title": "Next Generation Workbench integration into AWS ParallelCluster",
    "section": "3.2 Custom AMI",
    "text": "3.2 Custom AMI"
  },
  {
    "objectID": "PWB.html#sec-aws-parallelcluster-build",
    "href": "PWB.html#sec-aws-parallelcluster-build",
    "title": "Next Generation Workbench integration into AWS ParallelCluster",
    "section": "3.3 AWS ParallelCluster",
    "text": "3.3 AWS ParallelCluster\n\n3.3.1 Introduction\nWith launching the cluster via AWS ParallelCluster, everything comes together.\n\n\n3.3.2 Python Virtual Env\nThe virtual environment for AWS Parallelcluster can be created from the base folder of the git repo via\npython -m venv .aws-pc-venv\nsource .aws-pc-venv/bin/activate \npip install -r requirements.txt\ndeactivate\nYou may want to add the patch described in Section 4.1 to ensure full functionality of workbench.\n\n\n3.3.3 Prerequisites\n\nPython Virtual Environment set up and activated (cf. Section 3.3.2).\nAuxiliary Services up and running (cf. Section 3.1)\nCustom AMI built (cf. Section 3.2)\nS3 bucket set up for temporarily hosting cluster deployment files and scripts\n\n\n\n3.3.4 Deployment instructions\n\nReview the cluster template in config/cluster-config-wb.tmpl and modify accordingly.\nReview the deploy.sh script and modify accordingly, especially\n\nCLUSTERNAME - a human readable name of your cluster\nS3_BUCKETNAME - The name of the S3 bucket you set up in Section 3.3.3\nSECURITYGROUP_RSW - a security group that should allow at least external access to port 443 and 8787 (the latter if no SSL is being used).\nAMI - the AMI created in ?@sec-how-to-build-a-custom-ami\nSINGULARITY_SUPPORT - if set true, Workbench will be configured for Singularity integration and two r-session-complete containers (Ubuntu Jammy and Cent OS 7 based) will be built. Please note that this significantly extends the spin-up time of the cluster.\n\n\n\n\n3.3.5 Default values for Cluster deployment\nFor the deploy.sh script, unless mentioned in step 2 of the deployment instructions (cf. Section 3.3.4), all relevant parameters are extracted from the pulumi deployment for the auxiliary services.\nThe default value in the cluster template config/cluster-config-wb.tmpl` are as follows\n\nEFS storage used for shared file systems needed by AWS ParallelCluster\nOne Head Node\n\nInstance t3.xlarge\n100 GB of local EBS storage\nScript install-pwb-config.sh triggered when head node is being deployed.\n\nCompute Nodes with\n\nScript config-compute.sh triggered when compute node starts.\nPartition all\n\nInstance t3.xlarge\nminimum/maximum number of instances: 1/10\n\nPartition gpu\n\nInstance p3.2xlarge\nminimum/maximum number of instances; 0/1\n\n\n2 Login Nodes with\n\nInstance t3.xlarge\nELB in front\n\nShared storage for /home - FsX for Lustre with capacity of 1.2 TB and deployment type SCRATCH_2\n\nAll of the above settings (Instance type, numbers, FsX size) can be changed as needed.\n\n\n3.3.6 Notes on install-pwb-config.sh and install-compute.sh\ninstall-pwb-config.sh mainly creates Posit Workbench configuration files and configures the workbench systemctl services rstudio-launcher and rstudio-server . It is only executed on the designated head node\n\nWorkbench uses /opt/parallelcluster/shared/rstudio/ as the base for its configuration (PWB_BASE_DIR). /opt/parallelcluster/shared` is already created by AWS ParallelCluster and shared across all nodes (head, login and compute) so we are making use of this functionality.\nconfiguration files are deployed in $PWB_BASE_DIR/etc/rstudio\nshared storage is configured in $PWB_BASE_DIR/shared\nR Versions file is configured in $PWB_BASE_DIR/shared/r-versions\nIn order to distinguish the head node from the login node, an empty file /etc/head-node is created. This is used in the cron job mentioned in Section 3.2 to help differentiate the login nodes from the head node.\n\nìnstall-compute.sh script detects the presence of a GPU and then automatically updates the NVIDIA/CUDA driver and installs the CuDNN library for distributed GPU computing. This is more a nice to have but is rsther useful for distributed tensorflow etc…\n\n\n\nArchitecture diagram\n\n\n\n\n3.3.7 Customisations on top of AWS ParallelCluster\n\n3.3.7.1 Elastic Load Balancer\nAWS ParallelCluster is setting up an ELB for the Login nodes and ensures that the desired number of login nodes is available at any given time. The ELB is by default listening on port 22 (ssh). In order to change that one would need to patch the python scripts a bit (patch supplied in Section 4.1)\nThis change is simple but will effectively disable the ability to ssh into the ELB. Typically however Workbench Users do not need ssh access to login nodes - if needed, they can open a termina within the RStudio IDE, for example.\nAn alternative would be to add a second ELB for Workbench but this would imply a significantly larger patch to AWS ParallelCluster.\n\n\n3.3.7.2 The “thing” with the Login Nodes\nAWS ParallelCluster introduced the ability to define separate login nodes in Version 3.7.0. This is great and replaces a rather complicated workaround that was in place until then. Unfortunately the team did not add the same features to the new Login Nodes such as OnNodeConfigured . We have raise a github issue which was acknowledged and the missing feature will be implemented in an upcoming release.\nAs a consequence we have implemented a workaround with a cron job that runs on all ParallelCluster managed nodes (Login, Head and Compute) every minute. A login node is detected if there is a NFS mount that contains the name login_node and if there is no file /etc/head-node (the latter would signal that this is a head node indeed). See Section 3.3.6 for additional information.\nUntil the github issue is fixed, we will have to live with this workaround."
  },
  {
    "objectID": "PWB.html#summary-and-conclusions",
    "href": "PWB.html#summary-and-conclusions",
    "title": "Next Generation Workbench integration into AWS ParallelCluster",
    "section": "3.4 Summary and Conclusions",
    "text": "3.4 Summary and Conclusions\nThis document describes a possibility on how to integrate Workbench and AWS ParallelCluster that allows for partial High Availability. The setup can tolerate login node failures and recover and as a consequence the workbench part is HA.\nThe main ingredients for this setup is the creation of a custom AMI with all the software needed (Workbench, R, Python, …) baked into a custom AMI that can be used for all the three node types (Login, Head and Compute Node).\nIn order to achieve this, some additional logic has to be implemented and some workarounds for missing features in AWS ParallelCluster be used.\nThe remaining issue is however the single head node which is a single point of failure (if the head node crashes, SLURM stops working).\n\n3.4.1 How to reach “full” HA\nAWS paralelcluster makes a clear distinction between Head and Login nodes. This is more than justified given the fact that the Head node not only runs slurmctld but also can act as a NFS server exporting file systems such as /home , /opt/slurm, … This makes the Head node a single point of failure from the perspective of the NFS server alone.\nWith the release of AWS ParallelCluster 3.8.0 (currently available as beta version), all the NFS file systems can be hosted on external EFS. This removes the single point of failure for the NFS server. There is a bug in the beta version where all but one file system can be hosted on EFS but this will be fixed in the official release of 3.8.0.\nOnce this is in place, the boundaries between the Login Nodes and Head Nodes will become much less clear. With adding additional logic, one can automatically start additional slurmctld processes on the login nodes and configure those hosts in the slurm configuration. If the head node then fails, a slurmctld of one of the compute nodes will take over. While adding additional slurmctld is fairly straightforward, there also is a need for regular checks if all the defined slurmctld` hosts are still up and running. If not, those need to be removed from the slurm config.\nThe complexity of establishing the above is fairly small but then it is another customisation we have to make and maintain. As long as this is only a posit internal solution, we should be ok.\nA drawback of having full HA as mentioned above however is that very likely the ParallelCluster API may become unuseable in case the head node is no longer available. Things like updating configuration and settings of the running cluster may no longer work. Whether this is needed in a productive cluster is another matter of debate."
  },
  {
    "objectID": "PWB.html#sec-patch-for-elb",
    "href": "PWB.html#sec-patch-for-elb",
    "title": "Next Generation Workbench integration into AWS ParallelCluster",
    "section": "4.1 Patch for ELB to listen on port 8787 instead of 22",
    "text": "4.1 Patch for ELB to listen on port 8787 instead of 22\ndiff -u --recursive pcluster/templates/cluster_stack.py pcluster.new/templates/cluster_stack.py\n--- pcluster/templates/cluster_stack.py 2023-11-22 12:25:53\n+++ pcluster.new/templates/cluster_stack.py 2023-11-22 15:11:48\n@@ -871,10 +871,10 @@\n     def _get_source_ingress_rule(self, setting):\n         if setting.startswith(\"pl\"):\n             return ec2.CfnSecurityGroup.IngressProperty(\n-                ip_protocol=\"tcp\", from_port=22, to_port=22, source_prefix_list_id=setting\n+                ip_protocol=\"tcp\", from_port=8787, to_port=8787, source_prefix_list_id=setting\n             )\n         else:\n-            return ec2.CfnSecurityGroup.IngressProperty(ip_protocol=\"tcp\", from_port=22, to_port=22, cidr_ip=setting)\n+            return ec2.CfnSecurityGroup.IngressProperty(ip_protocol=\"tcp\", from_port=8787, to_port=8787, cidr_ip=setting)\n \n     def _add_login_nodes_security_group(self):\n         login_nodes_security_group_ingress = [\ndiff -u --recursive pcluster/templates/login_nodes_stack.py pcluster.new/templates/login_nodes_stack.py\n--- pcluster/templates/login_nodes_stack.py 2023-11-22 12:25:53\n+++ pcluster.new/templates/login_nodes_stack.py 2023-11-22 15:11:19\n@@ -273,10 +273,10 @@\n             self,\n             f\"{self._pool.name}TargetGroup\",\n             health_check=elbv2.HealthCheck(\n-                port=\"22\",\n+                port=\"8787\",\n                 protocol=elbv2.Protocol.TCP,\n             ),\n-            port=22,\n+            port=8787,\n             protocol=elbv2.Protocol.TCP,\n             target_type=elbv2.TargetType.INSTANCE,\n             vpc=self._vpc,\n@@ -299,7 +299,7 @@\n             ),\n         )\n \n-        listener = login_nodes_load_balancer.add_listener(f\"LoginNodesListener{self._pool.name}\", port=22)\n+        listener = login_nodes_load_balancer.add_listener(f\"LoginNodesListener{self._pool.name}\", port=8787)\n         listener.add_target_groups(f\"LoginNodesListenerTargets{self._pool.name}\", target_group)\n         return login_nodes_load_balancer"
  },
  {
    "objectID": "Introduction.html",
    "href": "Introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "1 Introduction\nAWS ParallelCluster is a framework that allows for easy setup of HPC clusters with various schedulers. It takes a YAML file with all the necessary definitions and transforms that into Cloud Formation Code that then gets deployed into AWS.\nAt the moment it supports both SLURM and AWS Batch.\nPosit Workbench supports SLURM as a HPC back end via the SLURM Launcher. As a consequence, a github repository has been set up to highlight a possible way to integrate Posit Workbench with AWS ParallelCluster via the SLURM Launcher. The approach used there works but has several shortcomings:\n\nSetting Workbench on the head node where also the slurmctld and slurmdbd (SLURM Controller and Database) daemons are running makes this head node very vulnerable and a single point of failure.\nAll traffic will be routed through the head node\nThe head node does not only act as Workbench Server and runs the main SLURM daemons (see above), it is also used as a NFS server adding additional load (depending on the size and utilisation of the cluster) that could contribute to very bad user experience on the HPC cluster up to a crash of the same if resources are exhausted.\n\nThis document serves two purposes: Documenting the current setup used for Workbench benchmarking but also summarizing a potential reference architecture that overcomes some of the shortcomings of the current Workbench integration into AWS ParallelCluster with a focus on High(er) Availabiity.\n\n\n2 A new approach\nFortunately, AWS ParallelCluster keeps evolving and in parallel Posit’s understanding of the tool also increases.\nRecent releases have added a couple of very interesting, exciting and very helpful features, most notably\n\nthe ability to add login nodes in version 3.7.0\nability to use EFS instead of NFS to host shared file systems needed for the cluster (e.g. /opt/slurm containing the SLURM installation) that removes the need to host an NFS server on the head node. This feature will be part of version 3.8.0 (beta version out - release imminent)\nability to set up /home on FsX for Lustre or EFS instead of internal NFS hosted on the head node This feature is part of version 3.8.0 as well.\n\nWhile the above features are very vital to the new approach for the Workbench integration discussed in this doc, there is many other functionalities that are almost taken for granted (e.g. Easy integration into auth subsystems, SLURM scheduling fine tuning capabilities, …).\nIf there was one feature that should be explicitly mentioned here, then it needs to be the ability to build custom AMIs. One of the features of a scalable cloud deployment (like AWS ParallelCluster) is the ability to scale up and down based on user demand. If there is a scale-up event, i.e. a node is getting added to the cluster, a new EC2 instance is provisioned. The elapsed time for such a scale-up event is about 4 minutes today when using a pre-built AMI but will increase if there is a need to run additional software installations just because the AMI does not contain all the needed features. Building a custom AMI will help to keep the instance spin up time at the 4 minute mark.\n\n\n3 Setup Instructions\nIn order to setup the new integration, 3 steps are needed\n\nCreate a custom AMI\nSet up auxiliary services (Active Directory, PostgreSQL DB, users) via Pulumi\nAWS ParallelCluster build AWS Parallelcluster\n\nAll those three steps are explained in the subsequent sections."
  }
]