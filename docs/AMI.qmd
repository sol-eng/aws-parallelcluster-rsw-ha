---
title: "Custom AMI"
number-sections: true
--- 

# Introduction 

While AWS ParallelCluster allows the on-demand provisioning of software during cluster build and compute node spin up, the waiting time especially until a compute node is spun up and provisioned quickly becomes incompatible with a positive user experience. The use of a custom AMI is very helpful to reduce this time. The spin up time however cannot be reduced below approximately 4 to 5 minutes which is the minimum time needed to bring up an EC2 instance and let AWS parallelcluster configure it. In a previous system where all software on the compute node was deployed on-demand on a default AMI, the spin-up time was larger than 10 minutes.

# Implementation details

Here we are using ParallelCluster's image builder capability that is integrated into AWS'  `packer` service.

## Main configuration

The main configuration file is `image-config.yaml` in the `image` subfolder. The idea for the custom image is to use a very slim approach. The yaml file references a script `install-image.sh` (cf. @sec-install-image-sh) that calls a couple of additional scripts that facilitate the installation of R, Python, quarto and apptainer. Each individual script, stored in an S3 bucket, is called by the `setup_something()` bash function that will download the additional script and run with pre-defined parameters that define the selected list of versions for the respective software. 

## R and Python configuration 

The R and Python installation scripts allow the installation of multiple [Python](https://docs.posit.co/resources/install-python.html) and [R](https://docs.posit.co/resources/install-r.html) versions using the Posit provided binaries. Each default version is symlinked into `/usr/local/bin`. 

Each R version (achieved by [`run.R`](../../image/run.R))

* is configured to use [public Posit Package Manager](https://packagemanager.posit.co) 
* is configured to use CRAN and Bioconductor repositories
* has all R packages needed for the RStudio IDE integration preinstalled into a site-library
* uses a time-based snapshot for CRAN that points to the snapshot that is at most 60 days older than the corresponding R release date
* uses a global `renv` cachethat  points to `/home/renv` 
* has the `renv`-`pak` integration enabled

The Python versions are setup to 

* include the `jupyter`/`jupyterlab` integration 
* include the capability to publish to Posit Connect
* use `pypi` from [public Posit Package Manager](https://packagemanager.posit.co)

## An important cron job 
In `install-image.sh` we also set up a cron job that will run `/opt/rstudio/scripts/rc.pwb`. When using the AMI, the cron job will check every minute whether the AMI is used on a login node or not. If on a login node, it will activate and start `rstudio-server` as well as `rstudio-launcher`. This has become necessary because AWS ParallelCluster does not support triggering of scripts upon the launch of login nodes (cf. https://github.com/aws/aws-parallelcluster/issues/5723)

# How to build a custom AMI 

## Prerequisites 

You need to have access to a S3 bucket. In case you don't have one available, you can create one using the AWS CLI, for example: 

``` bash
aws s3api create-bucket --bucket <BUCKETNAME> \
    --region <REGION> \
    --create-bucket-configuration LocationConstraint=<REGION>
```

where `<BUCKETNAME>` is your desired name of the S3 bucket and `<REGION>` the AWS region you would like to deploy this bucket to. 

## Step by step instruction 

In the github repo, go tho the [`image`](../../image) sub-folder. 

1. Make sure you have read/write access to a S3 bucket where you can store transient data and scripts. This bucket is referenced as `<BUCKETNAME>` hereafter. 
2. Identify the appropriate `pcluster` AMI (cf. @sec-pcluster-ami) and update `ParentImage` in `image-config.yaml`
3. Check versions in `install-image.sh` (cf. @sec-install-image-sh) and adjust accordingly
4. Finally, run 
``` bash
./build-image.sh <IMAGENAME> [<BUCKETNAME>]
```
where `<IMAGENAME>` is the desired name of the new AMI and `<BUCKETNAME>` the name of the S3 bucket, e.g. `hpc-scripts1234`. `<BUCKETNAME>` is an optional argument. If missing, the script will look in `.bucket.default` to read the default bucket name. 

# Other useful information for debugging etc.

## AMI Cleanup 

If you want to get rid of all AVAILABLE images, run

``` bash
for i in `pcluster list-images --image-status AVAILABLE | grep imageId | \
    awk '{print $2}' | sed 's#"##g' | sed 's#,##'`; \
        do pcluster delete-image -i $i ; done
```

## Get information about image 

``` bash
pcluster describe-image -i <IMAGENAME>
pcluster list-image-log-streams -i <IMAGENAME>
pcluster get-image-log-events  -i <IMAGENAME> --log-stream-name <AWSPCVERSION>/1
```

where `<AWSPCVERSION>` is the version of AWS parallelcluster used (e.g. 3.11.1) and `<IMAGENAME>` is the custom name of your AMI. 

## Sample `install-image.sh` script {#sec-install-image-sh}

This is just the start of the script. 

``` bash
R_VERSION_LIST="4.4.2 4.3.3 4.2.3 4.1.3 4.0.5"
R_VERSION_DEFAULT=4.4.2

PYTHON_VERSION_LIST="3.12.7 3.11.10 3.10.15"
PYTHON_VERSION_DEFAULT=3.12.7

QUARTO_VERSION=1.6.34

APPTAINER_VERSION="1.3.5"
```

## Find out the ID of the desired `pcluster` AMI {#sec-pcluster-ami}

You can check the AMI ID for the supported OS (Ubuntu 22.04 LTS (Jammy)) and architecture (x86_64) via 

``` bash 
pcluster list-official-images | \
    jq '.images.[] | 
        select(.os=="ubuntu2204" and .architecture=="x86_64") | .amiId'
```

## Find the latest default Ubuntu 22.04 AMI 

``` bash
aws ec2 describe-images \
    --owners 099720109477 \
    --filters "Name=name,Values=ubuntu/images/hvm-ssd/ubuntu-jammy-22.04-amd64-server-*" \
              "Name=architecture,Values=x86_64" \
              "Name=virtualization-type,Values=hvm" \
    --query 'Images[*].[CreationDate,ImageId]' \
    --output text | sort | tail -1 | awk '{print $2}'
```