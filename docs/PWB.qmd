---
title: "Next Generation Workbench integration into AWS ParallelCluster"
number-sections: true
---

# Introduction

[AWS ParallelCluster](https://aws.amazon.com/hpc/parallelcluster/) is a framework that allows for easy setup of HPC clusters with various schedulers. It takes a YAML file with all the necessary definitions and transforms that into Cloud Formation Code that then gets deployed into AWS.

At the moment it supports both [SLURM](https://slurm.schedmd.com/) and [AWS Batch](https://aws.amazon.com/batch/).

Posit Workbench supports SLURM as a HPC back end via the [SLURM Launcher](https://docs.posit.co/ide/server-pro/job_launcher/slurm_plugin.html). As a consequence, a [github repository](https://github.com/sol-eng/aws-parallelcluster-rsw/) has been set up to highlight a possible way to integrate Posit Workbench with AWS ParallelCluster via the SLURM Launcher. The approach used there works but has several shortcomings:

-   Setting Workbench on the head node where also the `slurmctld` and `slurmdbd` (SLURM Controller and Database) daemons are running makes this head node very vulnerable and a single point of failure.

-   All traffic will be routed through the head node

-   The head node does not only act as Workbench Server and runs the main SLURM daemons (see above), it is also used as a NFS server adding additional load (depending on the size and utilisation of the cluster) that could contribute to very bad user experience on the HPC cluster up to a crash of the same if resources are exhausted.

This document serves two purposes: Documenting the current setup used for Workbench benchmarking but also summarizing a potential reference architecture that overcomes some of the shortcomings of the current Workbench integration into AWS ParallelCluster with a focus on High(er) Availabiity.

# A new approach

Fortunately, AWS ParallelCluster keeps evolving and in parallel Posit's understanding of the tool also increases.

Recent releases have added a couple of very interesting, exciting and very helpful features, most notably

-   the ability to [add login nodes in version 3.7.0](https://github.com/aws/aws-parallelcluster/releases/tag/v3.7.0)

-   ability to use EFS instead of NFS to host shared file systems needed for the cluster (e.g. `/opt/slurm` containing the SLURM installation) that removes the need to host an NFS server on the head node. This feature will be part of [version 3.8.0](https://github.com/aws/aws-parallelcluster/blob/develop/CHANGELOG.md#380) (beta version out - release imminent)

-   ability to set up `/home` on FsX for Lustre or EFS instead of internal NFS hosted on the head node This feature is part of [version 3.8.0](https://github.com/aws/aws-parallelcluster/blob/develop/CHANGELOG.md#380) as well.

While the above features are very vital to the new approach for the Workbench integration discussed in this doc, there is many other functionalities that are almost taken for granted (e.g. Easy integration into auth subsystems, SLURM scheduling fine tuning capabilities, ...).

If there was one feature that should be explicitly mentioned here, then it needs to be the ability to build custom AMIs. One of the features of a scalable cloud deployment (like AWS ParallelCluster) is the ability to scale up and down based on user demand. If there is a scale-up event, i.e. a node is getting added to the cluster, a new EC2 instance is provisioned. The elapsed time for such a scale-up event is about 4 minutes today when using a pre-built AMI but will increase if there is a need to run additional software installations just because the AMI does not contain all the needed features. Building a custom AMI will help to keep the instance spin up time at the 4 minute mark.

# Setup Instructions

In order to setup the new integration, 3 steps are needed

-   Set up auxiliary services (Active Directory, PostgreSQL DB, users) - cf. @sec-auxiliary-services

-   Create a custom AMI (@sec-custom-ami)

-   Trigger AWS ParallelCluster build (@sec-aws-parallelcluster-build)

All those three steps are explained in the subsequent sections.

## Auxiliary services {#sec-auxiliary-services}

When using Posit Workbench for High Availability, the use of a PostgreSQL db is mandatory. Given the distributed nature of a HPC cluster, some kind of directory service for user management is needed. The directory service of choice here is [AWS SimpleAD](https://docs.aws.amazon.com/directoryservice/latest/admin-guide/directory_simple_ad.html). In order to efficiently and reliably work with this directory service, an additional EC2 instance is spun up that is used to add new users to the directory. This so-called jump host is fully integrated into SimpleAD and runs a tool called [adcli](https://www.freedesktop.org/software/realmd/adcli/adcli.html) (Active Directory CLI tool) that facilitates the management of users in SimpleAD. Via the use of additional `expect` scripts, this tool is used to programmatically create users. All of those tools and services are orchestrated via [Pulumi](https://www.pulumi.com/b/) recipes

### Prerequisites

You will need to have

-   pulumi installed and configure so you can successfully create, run and modify pulumi stacks

-   [just](https://github.com/casey/just) installed locally

-   ssh client including the `ssh-keygen` utility

### How to setup

In the github repo, go tho the `pulumi` sub-folder. There, run the following commands

``` bash
# Let's add a new ssh key pair
just key-pair-new

# Create a new stack 
pulumi stack init auxiliary-wb

# Configure eMail address to ensure resources are properly tagged
pulumi config set email my-email@corp.co

# add EC2 keypair via AWS CLI
aws ec2 import-key-pair --key-name `pulumi config get email`-keypair-for-pulumi --public-key-material `cat key.pem.pub| base64 ` 

# Finally start deployment of SimpleAD, PostgreSQL DB and Jump Host
# Also create 500 users at the same time
just up
```

Please be aware

-   Naming of your stack (`auxiliary-wb`) can be changed to your preference

-   Make sure to set your correct eMail address.

-   If you would like to use a different number of users, instead of `just up` run `pulumi up -y` and then `just create-users X` where X is the number of users you want to create.

-   You can change the default values for various parameters defined in `Pulumi.yaml` to your liking as well. Please do NOT change `Domain` - this is currently hard-coded into the AWS ParallelCluster setup. Anything else can be changed as you see fit.

Current configurable parameters in the pulumi recipe

| Parameter            | Description                                                          | Default value           |
|----------------------|----------------------------------------------------------------------|-------------------------|
| `region`             | AWS region                                                           | `eu-west-1`             |
| `email`              | eMail address of user                                                | `tbd@tbc.com`           |
| `ServerInstanceType` | Instance Type for the AD jumphost                                    | `t3.medium`             |
| `ami`                | A valid AMI used to deploy on AD jumphost (must be Ubuntu 20.04 LTS) | `ami-0d2a4a5d69e46ea0b` |
| `Domain`             | Name of Domain to be used for AD                                     | `pwb.posit.co`          |
| `DomainPW`           | Password for the Administrator AD account                            | `Testme123!`            |
| `db_username`        | User name for PostgreSQL DB                                          | `pwb_db_admin`          |
| `db_password`        | Password for PostgreSQL DB                                           | `pwb_db_password`       |

: Pulumi recipe parameters

Once you successfully built everything, `pulumi stack output`\` should report something like

``` bash
Current stack outputs (12):
    OUTPUT                   VALUE
    DomainPWARN              arn:aws:secretsmanager:eu-west-1:637485797898:secret:SimpleADPassword-2898387-BQn4mT
    ad_access_url            d-93675e652d.awsapps.com
    ad_dns_1                 172.31.33.122
    ad_dns_2                 172.31.48.170
    ad_jump_host_public_dns  ec2-52-16-178-244.eu-west-1.compute.amazonaws.com
    ad_jump_host_public_ip   52.16.178.244
    db_address               rsw-dbfee1a4f.clovh3dmuvji.eu-west-1.rds.amazonaws.com
    db_endpoint              rsw-dbfee1a4f.clovh3dmuvji.eu-west-1.rds.amazonaws.com:5432
    db_port                  5432
    jump_host_dns            ec2-52-16-178-244.eu-west-1.compute.amazonaws.com
    key_pair id              michael.mayer@posit.co-keypair-for-pulumi-1699956356
    vpc_subnet               subnet-03259a81db5aec449
```

### Additional details

Users are created in the following way by default: User Name is `positXXXX` where `XXXX` is a 4-digit zero-padded number. Password is `Testme1234`. Those defaults can be changed in `server-side-files/config/useradd.sh` . The referenced script is using multi-threaded bash to speed up user creation. In order to prevent user creation from failing due to too many concurrent connections, it additionally runs `pamtester` to ensure the user is correctly created.

## Custom AMI {#sec-custom-ami}




